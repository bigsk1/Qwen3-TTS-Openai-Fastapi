version: '3.8'

services:
  # GPU-enabled service with official backend (default)
  qwen3-tts-gpu:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    container_name: qwen3-tts-api
    network_mode: host
    ports:
      - "8881:8881"
    environment:
      - HOST=0.0.0.0
      - PORT=8881
      - WORKERS=1
      - CORS_ORIGINS=*
      - TTS_BACKEND=official
      - TTS_WARMUP_ON_START=false
      # Use Base model for voice cloning support
      - TTS_MODEL_NAME=Qwen/Qwen3-TTS-12Hz-1.7B-Base
      # Path to custom voice samples inside container
      - VOICE_SAMPLES_DIR=/app/voice-samples
      # Auto-unload model after N minutes of inactivity to free VRAM (0=disabled)
      # Useful when sharing GPU with other apps like Ollama, Kokoro TTS, etc.
      - TTS_INACTIVITY_TIMEOUT_MINUTES=15
      # When using device_ids filter, the GPU appears as device 0 inside container
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      # Mount model cache for persistence
      - ~/.cache/huggingface:/home/appuser/.cache/huggingface
      # Mount custom voice samples for voice cloning
      - ./sample-voices-xtts:/app/voice-samples:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8881/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # GPU-enabled service with vLLM-Omni backend
  qwen3-tts-vllm:
    build:
      context: .
      dockerfile: Dockerfile.vllm
    container_name: qwen3-tts-api-vllm
    network_mode: host
    ports:
      - "8880:8880"
    environment:
      - HOST=0.0.0.0
      - PORT=8880
      - WORKERS=1
      - CORS_ORIGINS=*
      - TTS_BACKEND=vllm_omni
      - TTS_WARMUP_ON_START=true
      - TTS_MODEL_NAME=Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_HOME=/root/.cache/huggingface
    volumes:
      # Mount model cache for persistence
      - ~/.cache/huggingface:/root/.cache/huggingface
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8880/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    profiles:
      - vllm

  # CPU-only service
  qwen3-tts-cpu:
    build:
      context: .
      dockerfile: Dockerfile
      target: cpu-base
    container_name: qwen3-tts-api-cpu
    network_mode: host
    ports:
      - "8880:8880"
    environment:
      - HOST=0.0.0.0
      - PORT=8880
      - WORKERS=1
      - CORS_ORIGINS=*
      - TTS_BACKEND=official
    volumes:
      - ~/.cache/huggingface:/home/appuser/.cache/huggingface
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8880/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    profiles:
      - cpu

# To run GPU version with official backend: docker-compose up qwen3-tts-gpu
# To run GPU version with vLLM backend: docker-compose --profile vllm up qwen3-tts-vllm
# To run CPU version: docker-compose --profile cpu up qwen3-tts-cpu
